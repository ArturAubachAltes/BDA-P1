{"cells":[{"cell_type":"markdown","metadata":{},"source":["Per entrar a una sessió de Spark i iniciar un builder en DeltaLake"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Ff5OllvaQ-i"},"outputs":[],"source":["#!pip install pyspark\n","#!pip install delta-spark\n","\n","import pyspark\n","from delta import *\n","\n","#!wget -O \"HR_comma_sep.csv\" \"https://mydisk.cs.upc.edu/s/3o33yciBHADiFCD/download/HR_comma_sep.csv\"\n","\n","builder = pyspark.sql.SparkSession.builder.appName(\"Shops_Deltalake\") \\\n","    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n","    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n","\n","spark = configure_spark_with_delta_pip(builder).getOrCreate()"]},{"cell_type":"markdown","metadata":{},"source":["## Comandes com si estessis a una sessió de Spark\n","\n","Llegir els arxius i guardar-los en format DeltaLake, que ens garantitza que es compleixen les restriccions ACID, i dona avanatatges \n","\n","\n","\n","\n","Carregar i llegir els 3 arxius de tenim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"inU0BE6yC-f-"},"outputs":[],"source":["#Arxiu Parquet \n","income = spark.read.parquet(\"./datalake/income_data/2024-04-17_IRSIncomeByZipCode_NoStateTotalsNoSmallZips.parquet\")\n","income.show()\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from pyspark.sql.functions import col\n","\n","# Lista de todos los caracteres inválidos que quieres reemplazar o eliminar\n","invalid_chars = [' ', ';', '{', '}', '(', ')', '\\n', '\\t', '=']\n","\n","# Función para limpiar los nombres de las columnas reemplazando los caracteres no válidos\n","def clean_column_name(column_name):\n","    for invalid_char in invalid_chars:\n","        column_name = column_name.replace(invalid_char, \"_\")  # Reemplaza por subrayado o cualquier otro caracter válido que prefieras\n","    return column_name\n","\n","# Aplicar la función de limpieza a cada columna\n","cleaned_income = income.select([col(c).alias(clean_column_name(c)) for c in income.columns])\n","\n","# Ahora guarda el DataFrame limpio en formato Delta\n","# income.write.mode(\"overwrite\").format(\"delta\").save(\"./deltalake/income_data/\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# finalitzar sessió de Spark\n","spark.stop()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN856KMaB+xcd49X0OG6wJE","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
