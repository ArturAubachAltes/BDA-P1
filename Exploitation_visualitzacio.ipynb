{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation Zone - Visualització\n",
    "\n",
    "- exploitation zone de la visualització\n",
    "- preparation pipeline per taula d'entrenament del model --> cada zipcode és un indiv\n",
    "    - Shops: latitu, longitud\n",
    "    - Income: mitjana income per zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/aina/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/aina/.ivy2/jars\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-460a2877-ac0f-43a0-8fdc-5f405b626994;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.12;3.1.0 in central\n",
      "\tfound io.delta#delta-storage;3.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      ":: resolution report :: resolve 177ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.12;3.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-460a2877-ac0f-43a0-8fdc-5f405b626994\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/11ms)\n",
      "24/04/23 11:01:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/23 11:01:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/04/23 11:01:52 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/04/23 11:01:52 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "#!pip install pyspark\n",
    "#!pip install delta-spark\n",
    "\n",
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "#!wget -O \"HR_comma_sep.csv\" \"https://mydisk.cs.upc.edu/s/3o33yciBHADiFCD/download/HR_comma_sep.csv\"\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"Shops_Deltalake\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arxiu Parquet \n",
    "shops = spark.read.parquet(\"./datalake/shops_data/2024-04-17_shops_data.parquet\")\n",
    "income = spark.read.parquet(\"./datalake/income_data/2024-04-22_IRSIncomeByZipCode_NoStateTotalsNoSmallZips.parquet\")\n",
    "sales = spark.read.parquet(\"./datalake/sales_data/2024-04-22_SuperstoreSalesTraining.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## INCOME ##\n",
    "############\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Lista de todos los caracteres inválidos que quieres reemplazar o eliminar\n",
    "invalid_chars = [' ', ';', '{', '}', '(', ')', '\\n', '\\t', '=']\n",
    "\n",
    "# Función para limpiar los nombres de las columnas reemplazando los caracteres no válidos\n",
    "def clean_column_name(column_name):\n",
    "    for invalid_char in invalid_chars:\n",
    "        column_name = column_name.replace(invalid_char, \"_\")  # Reemplaza por subrayado o cualquier otro caracter válido que prefieras\n",
    "    return column_name\n",
    "\n",
    "# Aplicar la función de limpieza a cada columna\n",
    "cleaned_income = income.select([col(c).alias(clean_column_name(c)) for c in income.columns])\n",
    "\n",
    "# seleccionem files que ens interessen per MODEL PREDICTIU\n",
    "income_selected = cleaned_income.select(\"ZIPCODE\", \"Total_income_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "## SHOPS ##\n",
    "###########\n",
    "# canviem nom columnes perque no ens deixa accedir-hi si tenen caracters especials\n",
    "# geometry.x\n",
    "new_column_name = \"geometry_x\"\n",
    "old_column_name = \"geometry.x\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# geometry.y\n",
    "new_column_name = \"geometry_y\"\n",
    "old_column_name = \"geometry.y\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.shop\n",
    "new_column_name = \"shop\"\n",
    "old_column_name = \"attributes.shop\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.name\n",
    "new_column_name = \"name\"\n",
    "old_column_name = \"attributes.name\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.osm_id2\n",
    "new_column_name = \"index\"\n",
    "old_column_name = \"attributes.osm_id2\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.addr_postcode\n",
    "new_column_name = \"postcode\"\n",
    "old_column_name = \"attributes.addr_postcode\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# ens quedem només amb columnes seleccionades\n",
    "selected_columns = ['geometry_x', 'geometry_y', \"shop\", \"name\", \"index\", \"postcode\"]\n",
    "shops_selected = shops.select(selected_columns)\n",
    "\n",
    "# eliminar files que tinguin missings --> excepte les que tenen missings a postcode!!\n",
    "shops_selected = shops_selected.filter(~(col(\"geometry_x\").isNull() |\n",
    "                                col(\"geometry_y\").isNull() |\n",
    "                                col(\"shop\").isNull() |\n",
    "                                col(\"name\").isNull() |\n",
    "                                col(\"index\").isNull()))\n",
    "\n",
    "# seleccionem files que ens interessen per MODEL PREDICTIU\n",
    "shops_selected = shops.select(\"shop\", \"postcode\", \"geometry_x\", \"geometry_y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+----------------------+\n",
      "|zipcode|latitud|longitud|avg_income_per_zipcode|\n",
      "+-------+-------+--------+----------------------+\n",
      "+-------+-------+--------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creem dataframe buit amb les columnes que volem que tingui\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# definim estructura\n",
    "schema = StructType([\n",
    "    StructField(\"zipcode\", StringType(), True),\n",
    "    StructField(\"latitud\", DoubleType(), True),\n",
    "    StructField(\"longitud\", DoubleType(), True),\n",
    "    StructField(\"avg_income_per_zipcode\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# crear dataframe buit\n",
    "df_visualitzacio = spark.createDataFrame([], schema)\n",
    "\n",
    "df_visualitzacio.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduim valors de zipcode\n",
    "df_visualitzaciogkmgu = df_visualitzacio.withColumn(\"zipcode\", income_selected[\"ZIPCODE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalitzar sessió de Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
