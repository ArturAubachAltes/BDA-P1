{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation Zone - Model Predictiu\n",
    "\n",
    "- exploitation zone del model predictiu\n",
    "- preparation pipeline per taula d'entrenament del model --> cada zipcode és un indiv\n",
    "    - Sales: 5 categories més comunes per zipcode, count vendes per zipcode, profit mitja per zipcode, mitjana num unitat per comanda per zipcode\n",
    "    - Shops: 5shops més comunes per zipcode\n",
    "    - Income: mitjana income per zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark\n",
    "#!pip install delta-spark\n",
    "\n",
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "#!wget -O \"HR_comma_sep.csv\" \"https://mydisk.cs.upc.edu/s/3o33yciBHADiFCD/download/HR_comma_sep.csv\"\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"Shops_Deltalake\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arxiu Parquet (de moment suposem aixo despres arreglem amb duckdb)\n",
    "shops = spark.read.parquet(\"./datalake/shops_data/2024-04-24_shops_data.parquet\")\n",
    "income = spark.read.parquet(\"./datalake/income_data/2024-04-24_IRSIncomeByZipCode_NoStateTotalsNoSmallZips.parquet\")\n",
    "sales = spark.read.parquet(\"./datalake/sales_data/2024-04-24_SuperstoreSalesTraining.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## INCOME ##\n",
    "############\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Lista de todos los caracteres inválidos que quieres reemplazar o eliminar\n",
    "invalid_chars = [' ', ';', '{', '}', '(', ')', '\\n', '\\t', '=']\n",
    "\n",
    "# Función para limpiar los nombres de las columnas reemplazando los caracteres no válidos\n",
    "def clean_column_name(column_name):\n",
    "    for invalid_char in invalid_chars:\n",
    "        column_name = column_name.replace(invalid_char, \"_\")  # Reemplaza por subrayado o cualquier otro caracter válido que prefieras\n",
    "    return column_name\n",
    "\n",
    "# Aplicar la función de limpieza a cada columna\n",
    "cleaned_income = income.select([col(c).alias(clean_column_name(c)) for c in income.columns])\n",
    "\n",
    "# seleccionem files que ens interessen per MODEL PREDICTIU\n",
    "income_selected = cleaned_income.select(\"ZIPCODE\", \"Total_income_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "## SALES ##\n",
    "###########\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Lista de todos los caracteres inválidos que quieres reemplazar o eliminar\n",
    "invalid_chars = [' ', ';', '{', '}', '(', ')', '\\n', '\\t', '=']\n",
    "\n",
    "# Función para limpiar los nombres de las columnas reemplazando los caracteres no válidos\n",
    "def clean_column_name(column_name):\n",
    "    for invalid_char in invalid_chars:\n",
    "        column_name = column_name.replace(invalid_char, \"_\")  # Reemplaza por subrayado o cualquier otro caracter válido que prefieras\n",
    "    return column_name\n",
    "\n",
    "# Aplicar la función de limpieza a cada columna\n",
    "cleaned_sales = sales.select([col(c).alias(clean_column_name(c)) for c in sales.columns])\n",
    "\n",
    "# filtrar EEUU\n",
    "sales_usa = cleaned_sales.filter(col(\"Country_/_Region\") == \"United States of America\")\n",
    "\n",
    "# eliminar row\n",
    "sales_usa = sales_usa.dropDuplicates(subset=[col for col in sales_usa.columns if col != \"row\"])\n",
    "\n",
    "# eliminar customer_name\n",
    "sales_usa = sales_usa.drop(\"Customer_Name\")\n",
    "\n",
    "# no fa falta fer imputació de missings perque quan filtrem per USA no ens queden columnes amb missings\n",
    "\n",
    "# eliminar missings a postal_code\n",
    "sales_usa = sales_usa.dropna(subset=[\"Postal_Code\"])\n",
    "\n",
    "# eliminar missings a subregions\n",
    "sales_usa = sales_usa.dropna(subset=[\"SubRegion\"])\n",
    "\n",
    "# eliminar files que missing a totes les columnes\n",
    "sales_usa = sales_usa.dropna(how=\"all\")\n",
    "\n",
    "\n",
    "# seleccionem files que ens interessen per MODEL PREDICTIU\n",
    "sales_selected = sales_usa.select(\"Postal_Code\", \"Category\", \"Sales\", \"Order_Quantity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "## SHOPS ##\n",
    "###########\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# canviem nom columnes perque no ens deixa accedir-hi si tenen caracters especials\n",
    "# geometry.x\n",
    "new_column_name = \"geometry_x\"\n",
    "old_column_name = \"geometry.x\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# geometry.y\n",
    "new_column_name = \"geometry_y\"\n",
    "old_column_name = \"geometry.y\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.shop\n",
    "new_column_name = \"shop\"\n",
    "old_column_name = \"attributes.shop\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.name\n",
    "new_column_name = \"name\"\n",
    "old_column_name = \"attributes.name\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.osm_id2\n",
    "new_column_name = \"index\"\n",
    "old_column_name = \"attributes.osm_id2\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.addr_postcode\n",
    "new_column_name = \"postcode\"\n",
    "old_column_name = \"attributes.addr_postcode\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# ens quedem només amb columnes seleccionades\n",
    "selected_columns = ['geometry_x', 'geometry_y', \"shop\", \"name\", \"index\", \"postcode\"]\n",
    "shops_selected = shops.select(selected_columns)\n",
    "\n",
    "# eliminar files que tinguin missings --> excepte les que tenen missings a postcode!!\n",
    "shops_selected = shops_selected.filter(~(col(\"geometry_x\").isNull() |\n",
    "                                col(\"geometry_y\").isNull() |\n",
    "                                col(\"shop\").isNull() |\n",
    "                                col(\"name\").isNull() |\n",
    "                                col(\"index\").isNull()))\n",
    "\n",
    "# seleccionem files que ens interessen per MODEL PREDICTIU\n",
    "shops_selected = shops.select(\"shop\", \"postcode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creem dataframe buit amb les columnes que volem que tingui\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# definim estructura\n",
    "schema = StructType([\n",
    "    StructField(\"zipcode\", StringType(), True),\n",
    "    StructField(\"avg_income_per_zipcode\", DoubleType(), True),\n",
    "    StructField(\"shop1\", StringType(), True),\n",
    "    StructField(\"shop2\", StringType(), True),\n",
    "    StructField(\"shop3\", StringType(), True),\n",
    "    StructField(\"shop4\", StringType(), True),\n",
    "    StructField(\"shop5\", StringType(), True),\n",
    "    StructField(\"sales_cat1\", StringType(), True),\n",
    "    StructField(\"sales_cat2\", StringType(), True),\n",
    "    StructField(\"sales_cat3\", StringType(), True),\n",
    "    StructField(\"sales_cat4\", StringType(), True),\n",
    "    StructField(\"sales_cat5\", StringType(), True),\n",
    "    StructField(\"sales_per_zipcode\", IntegerType(), True),\n",
    "    StructField(\"avg_profit_per_zipcode\", DoubleType(), True),\n",
    "    StructField(\"avg_order_quantity_per_zipcode\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# crear dataframe buit\n",
    "df_model = spark.createDataFrame([], schema)\n",
    "\n",
    "# df_model.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `zipcode` cannot be resolved. Did you mean one of the following? [`postcode`, `shop`].;\n'Aggregate ['zipcode, shop#905], ['zipcode, shop#905, count(1) AS count#1133L]\n+- Project [shop#905, postcode#974]\n   +- Project [attributes.objectid#558L, index#951, attributes.abandoned#560, attributes.addr_housename#561, attributes.addr_housenumber#562, attributes.addr_street#563, attributes.addr_city#564, attributes.addr_state#565, attributes.addr_postcode#566 AS postcode#974, attributes.addr_province#567, attributes.addr_country#568, attributes.addr_district#569, attributes.addr_subdistrict#570, attributes.addr_unit#571, attributes.amenity#572, attributes.brand#573, attributes.building#574, name#928, attributes.operator#576, shop#905, geometry_x#858, geometry_y#882]\n      +- Project [attributes.objectid#558L, attributes.osm_id2#559 AS index#951, attributes.abandoned#560, attributes.addr_housename#561, attributes.addr_housenumber#562, attributes.addr_street#563, attributes.addr_city#564, attributes.addr_state#565, attributes.addr_postcode#566, attributes.addr_province#567, attributes.addr_country#568, attributes.addr_district#569, attributes.addr_subdistrict#570, attributes.addr_unit#571, attributes.amenity#572, attributes.brand#573, attributes.building#574, name#928, attributes.operator#576, shop#905, geometry_x#858, geometry_y#882]\n         +- Project [attributes.objectid#558L, attributes.osm_id2#559, attributes.abandoned#560, attributes.addr_housename#561, attributes.addr_housenumber#562, attributes.addr_street#563, attributes.addr_city#564, attributes.addr_state#565, attributes.addr_postcode#566, attributes.addr_province#567, attributes.addr_country#568, attributes.addr_district#569, attributes.addr_subdistrict#570, attributes.addr_unit#571, attributes.amenity#572, attributes.brand#573, attributes.building#574, attributes.name#575 AS name#928, attributes.operator#576, shop#905, geometry_x#858, geometry_y#882]\n            +- Project [attributes.objectid#558L, attributes.osm_id2#559, attributes.abandoned#560, attributes.addr_housename#561, attributes.addr_housenumber#562, attributes.addr_street#563, attributes.addr_city#564, attributes.addr_state#565, attributes.addr_postcode#566, attributes.addr_province#567, attributes.addr_country#568, attributes.addr_district#569, attributes.addr_subdistrict#570, attributes.addr_unit#571, attributes.amenity#572, attributes.brand#573, attributes.building#574, attributes.name#575, attributes.operator#576, attributes.shop#577 AS shop#905, geometry_x#858, geometry_y#882]\n               +- Project [attributes.objectid#558L, attributes.osm_id2#559, attributes.abandoned#560, attributes.addr_housename#561, attributes.addr_housenumber#562, attributes.addr_street#563, attributes.addr_city#564, attributes.addr_state#565, attributes.addr_postcode#566, attributes.addr_province#567, attributes.addr_country#568, attributes.addr_district#569, attributes.addr_subdistrict#570, attributes.addr_unit#571, attributes.amenity#572, attributes.brand#573, attributes.building#574, attributes.name#575, attributes.operator#576, attributes.shop#577, geometry_x#858, geometry.y#579 AS geometry_y#882]\n                  +- Project [attributes.objectid#558L, attributes.osm_id2#559, attributes.abandoned#560, attributes.addr_housename#561, attributes.addr_housenumber#562, attributes.addr_street#563, attributes.addr_city#564, attributes.addr_state#565, attributes.addr_postcode#566, attributes.addr_province#567, attributes.addr_country#568, attributes.addr_district#569, attributes.addr_subdistrict#570, attributes.addr_unit#571, attributes.amenity#572, attributes.brand#573, attributes.building#574, attributes.name#575, attributes.operator#576, attributes.shop#577, geometry.x#578 AS geometry_x#858, geometry.y#579]\n                     +- Relation [attributes.objectid#558L,attributes.osm_id2#559,attributes.abandoned#560,attributes.addr_housename#561,attributes.addr_housenumber#562,attributes.addr_street#563,attributes.addr_city#564,attributes.addr_state#565,attributes.addr_postcode#566,attributes.addr_province#567,attributes.addr_country#568,attributes.addr_district#569,attributes.addr_subdistrict#570,attributes.addr_unit#571,attributes.amenity#572,attributes.brand#573,attributes.building#574,attributes.name#575,attributes.operator#576,attributes.shop#577,geometry.x#578,geometry.y#579] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m window \u001b[38;5;241m=\u001b[39m Window\u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzipcode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39morderBy(F\u001b[38;5;241m.\u001b[39mdesc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Calcular la cantidad de cada tipo de tienda para cada código postal\u001b[39;00m\n\u001b[0;32m      8\u001b[0m top_shops \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      9\u001b[0m     \u001b[43mshops_selected\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzipcode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m---> 10\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m          \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow_num\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mrow_number()\u001b[38;5;241m.\u001b[39mover(window))\n\u001b[0;32m     12\u001b[0m          \u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow_num\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     13\u001b[0m          \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzipcode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m          \u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzipcode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow_num\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m top_shops \u001b[38;5;241m=\u001b[39m top_shops\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Mostrar los resultados\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#top_shops.show()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\paula\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\group.py:38\u001b[0m, in \u001b[0;36mdfapi.<locals>._api\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_api\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroupedData\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m     37\u001b[0m     name \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m---> 38\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession)\n",
      "File \u001b[1;32mc:\\Users\\paula\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\paula\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `zipcode` cannot be resolved. Did you mean one of the following? [`postcode`, `shop`].;\n'Aggregate ['zipcode, shop#905], ['zipcode, shop#905, count(1) AS count#1133L]\n+- Project [shop#905, postcode#974]\n   +- Project [attributes.objectid#558L, index#951, attributes.abandoned#560, attributes.addr_housename#561, attributes.addr_housenumber#562, attributes.addr_street#563, attributes.addr_city#564, attributes.addr_state#565, attributes.addr_postcode#566 AS postcode#974, attributes.addr_province#567, attributes.addr_country#568, attributes.addr_district#569, attributes.addr_subdistrict#570, attributes.addr_unit#571, attributes.amenity#572, attributes.brand#573, attributes.building#574, name#928, attributes.operator#576, shop#905, geometry_x#858, geometry_y#882]\n      +- Project [attributes.objectid#558L, attributes.osm_id2#559 AS index#951, attributes.abandoned#560, attributes.addr_housename#561, attributes.addr_housenumber#562, attributes.addr_street#563, attributes.addr_city#564, attributes.addr_state#565, attributes.addr_postcode#566, attributes.addr_province#567, attributes.addr_country#568, attributes.addr_district#569, attributes.addr_subdistrict#570, attributes.addr_unit#571, attributes.amenity#572, attributes.brand#573, attributes.building#574, name#928, attributes.operator#576, shop#905, geometry_x#858, geometry_y#882]\n         +- Project [attributes.objectid#558L, attributes.osm_id2#559, attributes.abandoned#560, attributes.addr_housename#561, attributes.addr_housenumber#562, attributes.addr_street#563, attributes.addr_city#564, attributes.addr_state#565, attributes.addr_postcode#566, attributes.addr_province#567, attributes.addr_country#568, attributes.addr_district#569, attributes.addr_subdistrict#570, attributes.addr_unit#571, attributes.amenity#572, attributes.brand#573, attributes.building#574, attributes.name#575 AS name#928, attributes.operator#576, shop#905, geometry_x#858, geometry_y#882]\n            +- Project [attributes.objectid#558L, attributes.osm_id2#559, attributes.abandoned#560, attributes.addr_housename#561, attributes.addr_housenumber#562, attributes.addr_street#563, attributes.addr_city#564, attributes.addr_state#565, attributes.addr_postcode#566, attributes.addr_province#567, attributes.addr_country#568, attributes.addr_district#569, attributes.addr_subdistrict#570, attributes.addr_unit#571, attributes.amenity#572, attributes.brand#573, attributes.building#574, attributes.name#575, attributes.operator#576, attributes.shop#577 AS shop#905, geometry_x#858, geometry_y#882]\n               +- Project [attributes.objectid#558L, attributes.osm_id2#559, attributes.abandoned#560, attributes.addr_housename#561, attributes.addr_housenumber#562, attributes.addr_street#563, attributes.addr_city#564, attributes.addr_state#565, attributes.addr_postcode#566, attributes.addr_province#567, attributes.addr_country#568, attributes.addr_district#569, attributes.addr_subdistrict#570, attributes.addr_unit#571, attributes.amenity#572, attributes.brand#573, attributes.building#574, attributes.name#575, attributes.operator#576, attributes.shop#577, geometry_x#858, geometry.y#579 AS geometry_y#882]\n                  +- Project [attributes.objectid#558L, attributes.osm_id2#559, attributes.abandoned#560, attributes.addr_housename#561, attributes.addr_housenumber#562, attributes.addr_street#563, attributes.addr_city#564, attributes.addr_state#565, attributes.addr_postcode#566, attributes.addr_province#567, attributes.addr_country#568, attributes.addr_district#569, attributes.addr_subdistrict#570, attributes.addr_unit#571, attributes.amenity#572, attributes.brand#573, attributes.building#574, attributes.name#575, attributes.operator#576, attributes.shop#577, geometry.x#578 AS geometry_x#858, geometry.y#579]\n                     +- Relation [attributes.objectid#558L,attributes.osm_id2#559,attributes.abandoned#560,attributes.addr_housename#561,attributes.addr_housenumber#562,attributes.addr_street#563,attributes.addr_city#564,attributes.addr_state#565,attributes.addr_postcode#566,attributes.addr_province#567,attributes.addr_country#568,attributes.addr_district#569,attributes.addr_subdistrict#570,attributes.addr_unit#571,attributes.amenity#572,attributes.brand#573,attributes.building#574,attributes.name#575,attributes.operator#576,attributes.shop#577,geometry.x#578,geometry.y#579] parquet\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Definir una ventana sobre la cual aplicar row_number()\n",
    "window = Window.partitionBy(\"zipcode\").orderBy(F.desc(\"count\"))\n",
    "\n",
    "# Calcular la cantidad de cada tipo de tienda para cada código postal\n",
    "top_shops = (\n",
    "    shops_selected.groupBy(\"zipcode\", \"shop\")\n",
    "         .count()\n",
    "         .withColumn(\"row_num\", F.row_number().over(window))\n",
    "         .filter(F.col(\"row_num\") <= 5)\n",
    "         .select(\"zipcode\", \"shop\", \"count\")\n",
    "         .orderBy(\"zipcode\", \"row_num\")\n",
    ")\n",
    "\n",
    "top_shops = top_shops.drop(\"count\")\n",
    "\n",
    "# Mostrar los resultados\n",
    "#top_shops.show()\n",
    "\n",
    "shops_cleaned = top_shops.dropna()\n",
    "#shops_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+\n",
      "|zipcode|avg_income_per_zipcode|\n",
      "+-------+----------------------+\n",
      "|  35004|                258024|\n",
      "|  35005|                129390|\n",
      "|  35006|                 58585|\n",
      "|  35007|                651350|\n",
      "|  35010|                382106|\n",
      "|  35014|                 67885|\n",
      "|  35016|                333226|\n",
      "|  35019|                 35392|\n",
      "|  35020|                262475|\n",
      "|  35022|                521539|\n",
      "|  35023|                480458|\n",
      "|  35031|                112152|\n",
      "|  35033|                 67437|\n",
      "|  35034|                 52030|\n",
      "|  35035|                 31542|\n",
      "|  35040|                359868|\n",
      "|  35042|                 96503|\n",
      "|  35043|                363943|\n",
      "|  35044|                124406|\n",
      "|  35045|                236772|\n",
      "+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# taula a partir de income\n",
    "df_model = income_selected\n",
    "\n",
    "# renombrem columnes\n",
    "df_model = df_model.withColumnRenamed(\"ZIPCODE\", \"zipcode\")\n",
    "df_model = df_model.withColumnRenamed(\"Total_income_amount\", \"avg_income_per_zipcode\")\n",
    "\n",
    "# renombrem columnes de zipcode a sales i shops per poder fer join\n",
    "shops_selected = shops_selected.withColumnRenamed(\"postcode\", \"zipcode\")\n",
    "sales_selected = sales_selected.withColumnRenamed(\"Postal_Code\", \"zipcode\")\n",
    "\n",
    "df_model.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+-------------+\n",
      "|zipcode|avg_income_per_zipcode|         shop|\n",
      "+-------+----------------------+-------------+\n",
      "|  96701|               1369267|      charity|\n",
      "|  96701|               1369267|      bicycle|\n",
      "|  96701|               1369267|  supermarket|\n",
      "|  96706|               1657633|  supermarket|\n",
      "|  96706|               1657633|      chemist|\n",
      "|  96708|                205389|variety_store|\n",
      "|  96708|                205389|      hammock|\n",
      "|  96712|                209977|         gift|\n",
      "|  96712|                209977|       tattoo|\n",
      "|  96712|                209977|      jewelry|\n",
      "|  96712|                209977|       sports|\n",
      "|  96712|                209977|      clothes|\n",
      "|  96713|                 30221|          art|\n",
      "|  96720|               1091080|      clothes|\n",
      "|  96720|               1091080|      outdoor|\n",
      "|  96720|               1091080|          art|\n",
      "|  96720|               1091080|  health_food|\n",
      "|  96720|               1091080|  supermarket|\n",
      "|  96722|                 84923|  hairdresser|\n",
      "|  96722|                 84923|  convenience|\n",
      "+-------+----------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_data = df_model.join(top_shops, (df_model[\"zipcode\"] == top_shops[\"zipcode\"]), \"inner\")\n",
    "selected_columns = [df_model[col] for col in df_model.columns] + [top_shops[col] for col in top_shops.columns if col not in [\"zipcode\"]]\n",
    "joined_data = joined_data.select(selected_columns)\n",
    "joined_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas: 117\n",
      "Número de columnas: 3\n"
     ]
    }
   ],
   "source": [
    "#Aquesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cosetes aux 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o212.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 4.0 failed 1 times, most recent failure: Lost task 5.0 in stage 4.0 (TID 9) (Paula executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:695)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:660)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:636)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:582)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:541)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:695)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:660)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:636)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:582)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:541)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Obtener el número de filas\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m num_rows \u001b[38;5;241m=\u001b[39m \u001b[43mdf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Obtener el número de columnas\u001b[39;00m\n\u001b[0;32m      5\u001b[0m num_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df_model\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[1;32mc:\\Users\\paula\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1238\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \n\u001b[0;32m   1218\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\paula\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\paula\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\paula\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o212.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 4.0 failed 1 times, most recent failure: Lost task 5.0 in stage 4.0 (TID 9) (Paula executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:695)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:660)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:636)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:582)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:541)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1570)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:701)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:745)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:695)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:660)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:636)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:582)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:541)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 34 more\r\n"
     ]
    }
   ],
   "source": [
    "# Obtener el número de filas\n",
    "num_rows = df_model.count()\n",
    "\n",
    "# Obtener el número de columnas\n",
    "num_columns = len(df_model.columns)\n",
    "\n",
    "# Mostrar el número de filas y columnas\n",
    "print(\"Número de filas:\", num_rows)\n",
    "print(\"Número de columnas:\", num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+----+\n",
      "|zipcode|avg_income_per_zipcode|shop|\n",
      "+-------+----------------------+----+\n",
      "|  35004|                258024|NULL|\n",
      "|  35005|                129390|NULL|\n",
      "|  35006|                 58585|NULL|\n",
      "|  35007|                651350|NULL|\n",
      "|  35010|                382106|NULL|\n",
      "|  35014|                 67885|NULL|\n",
      "|  35016|                333226|NULL|\n",
      "|  35019|                 35392|NULL|\n",
      "|  35020|                262475|NULL|\n",
      "|  35022|                521539|NULL|\n",
      "|  35023|                480458|NULL|\n",
      "|  35031|                112152|NULL|\n",
      "|  35033|                 67437|NULL|\n",
      "|  35034|                 52030|NULL|\n",
      "|  35035|                 31542|NULL|\n",
      "|  35040|                359868|NULL|\n",
      "|  35042|                 96503|NULL|\n",
      "|  35043|                363943|NULL|\n",
      "|  35044|                124406|NULL|\n",
      "|  35045|                236772|NULL|\n",
      "+-------+----------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# taula a partir de income\n",
    "df_model = income_selected\n",
    "\n",
    "# renombrem columnes\n",
    "df_model = df_model.withColumnRenamed(\"ZIPCODE\", \"zipcode\")\n",
    "df_model = df_model.withColumnRenamed(\"Total_income_amount\", \"avg_income_per_zipcode\")\n",
    "\n",
    "# renombrem columnes de zipcode a sales i shops per poder fer join\n",
    "shops_selected = shops_selected.withColumnRenamed(\"postcode\", \"zipcode\")\n",
    "sales_selected = sales_selected.withColumnRenamed(\"Postal_Code\", \"zipcode\")\n",
    "\n",
    "# modificar taula shops per que tingui les columnes que volem --> shop1, shop2, shop3, shop4, shop5\n",
    "\n",
    "\n",
    "df_model = df_model.join(shops_selected, \"zipcode\", \"left\")\n",
    "\n",
    "\n",
    "# modificar taula sales perque tingui les columnes que volem --> sales_cat1, sales_cat2, sales_cat3, sales_cat4, sales_cat5, sales_per_zipcode, avg_profit_per_zipcode, avg_order_quantity_per_zipcode\n",
    "\n",
    "\n",
    "df_model = df_model.join(sales_selected, \"zipcode\", \"left\")\n",
    "\n",
    "\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "df_model.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduim valors de zipcode\n",
    "df_model = df_model.withColumn(\"zipcode\", income_selected[\"ZIPCODE\"])\n",
    "\n",
    "# unim income amb df_model\n",
    "df_model = df_model.join(income_selected, df_model.zipcode == income_selected.zipcode, how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, count, desc, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Calcular la media de ingresos por código postal\n",
    "income_avg = income_data.groupBy(\"zipcode\").agg(avg(\"income\").alias(\"avg_income\"))\n",
    "\n",
    "# Determinar las tiendas más comunes por código postal\n",
    "window = Window.partitionBy(\"zipcode\").orderBy(desc(\"count\"))\n",
    "top_shops = sales_data.groupBy(\"zipcode\", \"shop\").agg(count(\"*\").alias(\"count\")).\\\n",
    "    withColumn(\"rn\", row_number().over(window)).filter(col(\"rn\") <= 5)\n",
    "\n",
    "# Calcular métricas de ventas por código postal y tienda\n",
    "sales_metrics = sales_data.groupBy(\"zipcode\", \"shop\", \"category\").\\\n",
    "    agg(count(\"*\").alias(\"count\"), avg(\"sales\").alias(\"avg_sales\"), avg(\"profit\").alias(\"avg_profit\"),\n",
    "        avg(\"num_units\").alias(\"avg_num_units\"))\n",
    "\n",
    "# Combinar todas las métricas en una sola tabla\n",
    "result_table = sales_metrics.join(income_avg, \"zipcode\", \"left\").\\\n",
    "    join(top_shops, [\"zipcode\", \"shop\"], \"left\")\n",
    "\n",
    "# Mostrar el resultado\n",
    "result_table.show()\n",
    "\n",
    "# Finalizar la sesión de Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalitzar sessió de Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
