{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation Zone - Model Predictiu\n",
    "\n",
    "- exploitation zone del model predictiu\n",
    "- preparation pipeline per taula d'entrenament del model --> cada zipcode és un indiv\n",
    "    - Sales: 5 categories més comunes per zipcode, count vendes per zipcode, profit mitja per zipcode, mitjana num unitat per comanda per zipcode\n",
    "    - Shops: 5shops més comunes per zipcode\n",
    "    - Income: mitjana income per zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark\n",
    "#!pip install delta-spark\n",
    "\n",
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "#!wget -O \"HR_comma_sep.csv\" \"https://mydisk.cs.upc.edu/s/3o33yciBHADiFCD/download/HR_comma_sep.csv\"\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"Shops_Deltalake\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arxiu Parquet (de moment suposem aixo despres arreglem amb duckdb)\n",
    "shops = spark.read.parquet(\"./datalake/shops_data/2024-04-24_shops_data.parquet\")\n",
    "income = spark.read.parquet(\"./datalake/income_data/2024-04-24_IRSIncomeByZipCode_NoStateTotalsNoSmallZips.parquet\")\n",
    "sales = spark.read.parquet(\"./datalake/sales_data/2024-04-24_SuperstoreSalesTraining.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## INCOME ##\n",
    "############\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Lista de todos los caracteres inválidos que quieres reemplazar o eliminar\n",
    "invalid_chars = [' ', ';', '{', '}', '(', ')', '\\n', '\\t', '=']\n",
    "\n",
    "# Función para limpiar los nombres de las columnas reemplazando los caracteres no válidos\n",
    "def clean_column_name(column_name):\n",
    "    for invalid_char in invalid_chars:\n",
    "        column_name = column_name.replace(invalid_char, \"_\")  # Reemplaza por subrayado o cualquier otro caracter válido que prefieras\n",
    "    return column_name\n",
    "\n",
    "# Aplicar la función de limpieza a cada columna\n",
    "cleaned_income = income.select([col(c).alias(clean_column_name(c)) for c in income.columns])\n",
    "\n",
    "# seleccionem files que ens interessen per MODEL PREDICTIU\n",
    "income_selected = cleaned_income.select(\"ZIPCODE\", \"Total_income_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "## SALES ##\n",
    "###########\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Lista de todos los caracteres inválidos que quieres reemplazar o eliminar\n",
    "invalid_chars = [' ', ';', '{', '}', '(', ')', '\\n', '\\t', '=']\n",
    "\n",
    "# Función para limpiar los nombres de las columnas reemplazando los caracteres no válidos\n",
    "def clean_column_name(column_name):\n",
    "    for invalid_char in invalid_chars:\n",
    "        column_name = column_name.replace(invalid_char, \"_\")  # Reemplaza por subrayado o cualquier otro caracter válido que prefieras\n",
    "    return column_name\n",
    "\n",
    "# Aplicar la función de limpieza a cada columna\n",
    "cleaned_sales = sales.select([col(c).alias(clean_column_name(c)) for c in sales.columns])\n",
    "\n",
    "# filtrar EEUU\n",
    "sales_usa = cleaned_sales.filter(col(\"Country_/_Region\") == \"United States of America\")\n",
    "\n",
    "# eliminar row\n",
    "sales_usa = sales_usa.dropDuplicates(subset=[col for col in sales_usa.columns if col != \"row\"])\n",
    "\n",
    "# eliminar customer_name\n",
    "sales_usa = sales_usa.drop(\"Customer_Name\")\n",
    "\n",
    "# no fa falta fer imputació de missings perque quan filtrem per USA no ens queden columnes amb missings\n",
    "\n",
    "# eliminar missings a postal_code\n",
    "sales_usa = sales_usa.dropna(subset=[\"Postal_Code\"])\n",
    "\n",
    "# eliminar missings a subregions\n",
    "sales_usa = sales_usa.dropna(subset=[\"SubRegion\"])\n",
    "\n",
    "# eliminar files que missing a totes les columnes\n",
    "sales_usa = sales_usa.dropna(how=\"all\")\n",
    "\n",
    "\n",
    "# seleccionem files que ens interessen per MODEL PREDICTIU\n",
    "sales_selected = sales_usa.select(\"Postal_Code\", \"Category\", \"Sales\", \"Order_Quantity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "## SHOPS ##\n",
    "###########\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# canviem nom columnes perque no ens deixa accedir-hi si tenen caracters especials\n",
    "# geometry.x\n",
    "new_column_name = \"geometry_x\"\n",
    "old_column_name = \"geometry.x\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# geometry.y\n",
    "new_column_name = \"geometry_y\"\n",
    "old_column_name = \"geometry.y\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.shop\n",
    "new_column_name = \"shop\"\n",
    "old_column_name = \"attributes.shop\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.name\n",
    "new_column_name = \"name\"\n",
    "old_column_name = \"attributes.name\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.osm_id2\n",
    "new_column_name = \"index\"\n",
    "old_column_name = \"attributes.osm_id2\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.addr_postcode\n",
    "new_column_name = \"postcode\"\n",
    "old_column_name = \"attributes.addr_postcode\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# ens quedem només amb columnes seleccionades\n",
    "selected_columns = ['geometry_x', 'geometry_y', \"shop\", \"name\", \"index\", \"postcode\"]\n",
    "shops_selected = shops.select(selected_columns)\n",
    "\n",
    "# eliminar files que tinguin missings --> excepte les que tenen missings a postcode!!\n",
    "shops_selected = shops_selected.filter(~(col(\"geometry_x\").isNull() |\n",
    "                                col(\"geometry_y\").isNull() |\n",
    "                                col(\"shop\").isNull() |\n",
    "                                col(\"name\").isNull() |\n",
    "                                col(\"index\").isNull()))\n",
    "\n",
    "# seleccionem files que ens interessen per MODEL PREDICTIU\n",
    "shops_selected = shops.select(\"shop\", \"postcode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+-----+-----+-----+-----+-----+----------+----------+----------+----------+----------+-----------------+----------------------+------------------------------+\n",
      "|zipcode|avg_income_per_zipcode|shop1|shop2|shop3|shop4|shop5|sales_cat1|sales_cat2|sales_cat3|sales_cat4|sales_cat5|sales_per_zipcode|avg_profit_per_zipcode|avg_order_quantity_per_zipcode|\n",
      "+-------+----------------------+-----+-----+-----+-----+-----+----------+----------+----------+----------+----------+-----------------+----------------------+------------------------------+\n",
      "+-------+----------------------+-----+-----+-----+-----+-----+----------+----------+----------+----------+----------+-----------------+----------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creem dataframe buit amb les columnes que volem que tingui\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# definim estructura\n",
    "schema = StructType([\n",
    "    StructField(\"zipcode\", StringType(), True),\n",
    "    StructField(\"avg_income_per_zipcode\", DoubleType(), True),\n",
    "    StructField(\"shop1\", StringType(), True),\n",
    "    StructField(\"shop2\", StringType(), True),\n",
    "    StructField(\"shop3\", StringType(), True),\n",
    "    StructField(\"shop4\", StringType(), True),\n",
    "    StructField(\"shop5\", StringType(), True),\n",
    "    StructField(\"sales_cat1\", StringType(), True),\n",
    "    StructField(\"sales_cat2\", StringType(), True),\n",
    "    StructField(\"sales_cat3\", StringType(), True),\n",
    "    StructField(\"sales_cat4\", StringType(), True),\n",
    "    StructField(\"sales_cat5\", StringType(), True),\n",
    "    StructField(\"sales_per_zipcode\", IntegerType(), True),\n",
    "    StructField(\"avg_profit_per_zipcode\", DoubleType(), True),\n",
    "    StructField(\"avg_order_quantity_per_zipcode\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# crear dataframe buit\n",
    "df_model = spark.createDataFrame([], schema)\n",
    "\n",
    "df_model.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----+\n",
      "|zipcode|        shop|count|\n",
      "+-------+------------+-----+\n",
      "|   NULL| convenience|  327|\n",
      "|   NULL|     clothes|  162|\n",
      "|   NULL| supermarket|  102|\n",
      "|   NULL|        gift|   64|\n",
      "|   NULL|         yes|   48|\n",
      "|  28000|     florist|    1|\n",
      "|  28000|    copyshop|    1|\n",
      "|  28000| supermarket|    1|\n",
      "|  28000|    boutique|    1|\n",
      "|  28000|dry_cleaning|    1|\n",
      "|  28017|     clothes|    4|\n",
      "|  28017| hairdresser|    3|\n",
      "|  28017|      beauty|    3|\n",
      "|  28017| convenience|    3|\n",
      "|  28017|       paint|    2|\n",
      "|  28020|  car_repair|    1|\n",
      "|  28035|       shoes|    2|\n",
      "|  28035|  stationery|    2|\n",
      "|  28035|     laundry|    2|\n",
      "|  28035|       paint|    2|\n",
      "+-------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "shops_selected = shops_selected.withColumnRenamed(\"postcode\", \"zipcode\")\n",
    "\n",
    "# Definir una ventana sobre la cual aplicar row_number()\n",
    "window = Window.partitionBy(\"zipcode\").orderBy(F.desc(\"count\"))\n",
    "\n",
    "# Calcular la cantidad de cada tipo de tienda para cada código postal\n",
    "top_shops = (\n",
    "    shops_selected.groupBy(\"zipcode\", \"shop\")\n",
    "         .count()\n",
    "         .withColumn(\"row_num\", F.row_number().over(window))\n",
    "         .filter(F.col(\"row_num\") <= 5)\n",
    "         .select(\"zipcode\", \"shop\", \"count\")\n",
    "         .orderBy(\"zipcode\", \"row_num\")\n",
    ")\n",
    "\n",
    "# Mostrar los resultados\n",
    "top_shops.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+\n",
      "|zipcode|avg_income_per_zipcode|\n",
      "+-------+----------------------+\n",
      "|  35004|                258024|\n",
      "|  35005|                129390|\n",
      "|  35006|                 58585|\n",
      "|  35007|                651350|\n",
      "|  35010|                382106|\n",
      "|  35014|                 67885|\n",
      "|  35016|                333226|\n",
      "|  35019|                 35392|\n",
      "|  35020|                262475|\n",
      "|  35022|                521539|\n",
      "|  35023|                480458|\n",
      "|  35031|                112152|\n",
      "|  35033|                 67437|\n",
      "|  35034|                 52030|\n",
      "|  35035|                 31542|\n",
      "|  35040|                359868|\n",
      "|  35042|                 96503|\n",
      "|  35043|                363943|\n",
      "|  35044|                124406|\n",
      "|  35045|                236772|\n",
      "+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# taula a partir de income\n",
    "df_model = income_selected\n",
    "\n",
    "# renombrem columnes\n",
    "df_model = df_model.withColumnRenamed(\"ZIPCODE\", \"zipcode\")\n",
    "df_model = df_model.withColumnRenamed(\"Total_income_amount\", \"avg_income_per_zipcode\")\n",
    "\n",
    "# renombrem columnes de zipcode a sales i shops per poder fer join\n",
    "shops_selected = shops_selected.withColumnRenamed(\"postcode\", \"zipcode\")\n",
    "sales_selected = sales_selected.withColumnRenamed(\"Postal_Code\", \"zipcode\")\n",
    "\n",
    "df_model.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|         shop|zipcode|\n",
      "+-------------+-------+\n",
      "|confectionery|   NULL|\n",
      "|      seafood|   NULL|\n",
      "|  supermarket|   NULL|\n",
      "|      laundry|   NULL|\n",
      "|      clothes|   NULL|\n",
      "|    cosmetics|   NULL|\n",
      "|  convenience|   NULL|\n",
      "|      clothes|   NULL|\n",
      "|      clothes|   NULL|\n",
      "|          art|   NULL|\n",
      "|      outdoor|   NULL|\n",
      "|    chocolate|   NULL|\n",
      "|       tattoo|   NULL|\n",
      "|  supermarket|   NULL|\n",
      "|      laundry|   NULL|\n",
      "|        books|   NULL|\n",
      "|     boutique|  96716|\n",
      "|       bakery|   NULL|\n",
      "|  supermarket|   NULL|\n",
      "|      outdoor|   NULL|\n",
      "+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shops_selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `attributes`.`objectid` cannot be resolved. Did you mean one of the following? [`attributes`.`objectid`, `index`, `attributes`.`abandoned`, `attributes`.`addr_housename`, `attributes`.`addr_housenumber`, `attributes`.`addr_street`, `attributes`.`addr_city`, `attributes`.`addr_state`, `postcode`, `attributes`.`addr_province`, `attributes`.`addr_country`, `attributes`.`addr_district`, `attributes`.`addr_subdistrict`, `attributes`.`addr_unit`, `attributes`.`amenity`, `attributes`.`brand`, `attributes`.`building`, `name`, `attributes`.`operator`, `shop`, `geometry_x`, `geometry_y`].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m shops_cleaned \u001b[38;5;241m=\u001b[39m \u001b[43mshops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m shops_cleaned\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\paula\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\dataframe.py:4349\u001b[0m, in \u001b[0;36mDataFrame.dropna\u001b[1;34m(self, how, thresh, subset)\u001b[0m\n\u001b[0;32m   4346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m thresh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4347\u001b[0m     thresh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(subset) \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 4349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32mc:\\Users\\paula\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\paula\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `attributes`.`objectid` cannot be resolved. Did you mean one of the following? [`attributes`.`objectid`, `index`, `attributes`.`abandoned`, `attributes`.`addr_housename`, `attributes`.`addr_housenumber`, `attributes`.`addr_street`, `attributes`.`addr_city`, `attributes`.`addr_state`, `postcode`, `attributes`.`addr_province`, `attributes`.`addr_country`, `attributes`.`addr_district`, `attributes`.`addr_subdistrict`, `attributes`.`addr_unit`, `attributes`.`amenity`, `attributes`.`brand`, `attributes`.`building`, `name`, `attributes`.`operator`, `shop`, `geometry_x`, `geometry_y`]."
     ]
    }
   ],
   "source": [
    "shops_cleaned = shops.dropna()\n",
    "shops_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+----+\n",
      "|zipcode|avg_income_per_zipcode|shop|\n",
      "+-------+----------------------+----+\n",
      "|  35004|                258024|NULL|\n",
      "|  35005|                129390|NULL|\n",
      "|  35006|                 58585|NULL|\n",
      "|  35007|                651350|NULL|\n",
      "|  35010|                382106|NULL|\n",
      "|  35014|                 67885|NULL|\n",
      "|  35016|                333226|NULL|\n",
      "|  35019|                 35392|NULL|\n",
      "|  35020|                262475|NULL|\n",
      "|  35022|                521539|NULL|\n",
      "|  35023|                480458|NULL|\n",
      "|  35031|                112152|NULL|\n",
      "|  35033|                 67437|NULL|\n",
      "|  35034|                 52030|NULL|\n",
      "|  35035|                 31542|NULL|\n",
      "|  35040|                359868|NULL|\n",
      "|  35042|                 96503|NULL|\n",
      "|  35043|                363943|NULL|\n",
      "|  35044|                124406|NULL|\n",
      "|  35045|                236772|NULL|\n",
      "+-------+----------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# taula a partir de income\n",
    "df_model = income_selected\n",
    "\n",
    "# renombrem columnes\n",
    "df_model = df_model.withColumnRenamed(\"ZIPCODE\", \"zipcode\")\n",
    "df_model = df_model.withColumnRenamed(\"Total_income_amount\", \"avg_income_per_zipcode\")\n",
    "\n",
    "# renombrem columnes de zipcode a sales i shops per poder fer join\n",
    "shops_selected = shops_selected.withColumnRenamed(\"postcode\", \"zipcode\")\n",
    "sales_selected = sales_selected.withColumnRenamed(\"Postal_Code\", \"zipcode\")\n",
    "\n",
    "# modificar taula shops per que tingui les columnes que volem --> shop1, shop2, shop3, shop4, shop5\n",
    "\n",
    "\n",
    "df_model = df_model.join(shops_selected, \"zipcode\", \"left\")\n",
    "\n",
    "\n",
    "# modificar taula sales perque tingui les columnes que volem --> sales_cat1, sales_cat2, sales_cat3, sales_cat4, sales_cat5, sales_per_zipcode, avg_profit_per_zipcode, avg_order_quantity_per_zipcode\n",
    "\n",
    "\n",
    "df_model = df_model.join(sales_selected, \"zipcode\", \"left\")\n",
    "\n",
    "\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "df_model.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduim valors de zipcode\n",
    "df_model = df_model.withColumn(\"zipcode\", income_selected[\"ZIPCODE\"])\n",
    "\n",
    "# unim income amb df_model\n",
    "df_model = df_model.join(income_selected, df_model.zipcode == income_selected.zipcode, how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, count, desc, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Calcular la media de ingresos por código postal\n",
    "income_avg = income_data.groupBy(\"zipcode\").agg(avg(\"income\").alias(\"avg_income\"))\n",
    "\n",
    "# Determinar las tiendas más comunes por código postal\n",
    "window = Window.partitionBy(\"zipcode\").orderBy(desc(\"count\"))\n",
    "top_shops = sales_data.groupBy(\"zipcode\", \"shop\").agg(count(\"*\").alias(\"count\")).\\\n",
    "    withColumn(\"rn\", row_number().over(window)).filter(col(\"rn\") <= 5)\n",
    "\n",
    "# Calcular métricas de ventas por código postal y tienda\n",
    "sales_metrics = sales_data.groupBy(\"zipcode\", \"shop\", \"category\").\\\n",
    "    agg(count(\"*\").alias(\"count\"), avg(\"sales\").alias(\"avg_sales\"), avg(\"profit\").alias(\"avg_profit\"),\n",
    "        avg(\"num_units\").alias(\"avg_num_units\"))\n",
    "\n",
    "# Combinar todas las métricas en una sola tabla\n",
    "result_table = sales_metrics.join(income_avg, \"zipcode\", \"left\").\\\n",
    "    join(top_shops, [\"zipcode\", \"shop\"], \"left\")\n",
    "\n",
    "# Mostrar el resultado\n",
    "result_table.show()\n",
    "\n",
    "# Finalizar la sesión de Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalitzar sessió de Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
