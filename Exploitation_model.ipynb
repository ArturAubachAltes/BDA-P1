{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation Zone - Model Predictiu\n",
    "\n",
    "- exploitation zone del model predictiu\n",
    "- preparation pipeline per taula d'entrenament del model --> cada zipcode és un indiv\n",
    "    - Sales: 5 categories més comunes per zipcode, count vendes per zipcode, profit mitja per zipcode, mitjana num unitat per comanda per zipcode\n",
    "    - Shops: 5shops més comunes per zipcode\n",
    "    - Income: mitjana income per zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark\n",
    "#!pip install delta-spark\n",
    "\n",
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "#!wget -O \"HR_comma_sep.csv\" \"https://mydisk.cs.upc.edu/s/3o33yciBHADiFCD/download/HR_comma_sep.csv\"\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"Shops_Deltalake\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arxiu Parquet (de moment suposem aixo despres arreglem amb duckdb)\n",
    "shops = spark.read.parquet(\"./datalake/shops_data/2024-04-17_shops_data.parquet\")\n",
    "income = spark.read.parquet(\"./datalake/income_data/2024-04-22_IRSIncomeByZipCode_NoStateTotalsNoSmallZips.parquet\")\n",
    "sales = spark.read.parquet(\"./datalake/sales_data/2024-04-22_SuperstoreSalesTraining.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## INCOME ##\n",
    "############\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Lista de todos los caracteres inválidos que quieres reemplazar o eliminar\n",
    "invalid_chars = [' ', ';', '{', '}', '(', ')', '\\n', '\\t', '=']\n",
    "\n",
    "# Función para limpiar los nombres de las columnas reemplazando los caracteres no válidos\n",
    "def clean_column_name(column_name):\n",
    "    for invalid_char in invalid_chars:\n",
    "        column_name = column_name.replace(invalid_char, \"_\")  # Reemplaza por subrayado o cualquier otro caracter válido que prefieras\n",
    "    return column_name\n",
    "\n",
    "# Aplicar la función de limpieza a cada columna\n",
    "cleaned_income = income.select([col(c).alias(clean_column_name(c)) for c in income.columns])\n",
    "\n",
    "# seleccionem files que ens interessen per MODEL PREDICTIU\n",
    "income_selected = cleaned_income.select(\"ZIPCODE\", \"Total_income_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "## SALES ##\n",
    "###########\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Lista de todos los caracteres inválidos que quieres reemplazar o eliminar\n",
    "invalid_chars = [' ', ';', '{', '}', '(', ')', '\\n', '\\t', '=']\n",
    "\n",
    "# Función para limpiar los nombres de las columnas reemplazando los caracteres no válidos\n",
    "def clean_column_name(column_name):\n",
    "    for invalid_char in invalid_chars:\n",
    "        column_name = column_name.replace(invalid_char, \"_\")  # Reemplaza por subrayado o cualquier otro caracter válido que prefieras\n",
    "    return column_name\n",
    "\n",
    "# Aplicar la función de limpieza a cada columna\n",
    "cleaned_sales = sales.select([col(c).alias(clean_column_name(c)) for c in sales.columns])\n",
    "\n",
    "# filtrar EEUU\n",
    "sales_usa = cleaned_sales.filter(col(\"Country_/_Region\") == \"United States of America\")\n",
    "\n",
    "# eliminar row\n",
    "sales_usa = sales_usa.dropDuplicates(subset=[col for col in sales_usa.columns if col != \"row\"])\n",
    "\n",
    "# eliminar customer_name\n",
    "sales_usa = sales_usa.drop(\"Customer_Name\")\n",
    "\n",
    "# no fa falta fer imputació de missings perque quan filtrem per USA no ens queden columnes amb missings\n",
    "\n",
    "# eliminar missings a postal_code\n",
    "sales_usa = sales_usa.dropna(subset=[\"Postal_Code\"])\n",
    "\n",
    "# eliminar missings a subregions\n",
    "sales_usa = sales_usa.dropna(subset=[\"SubRegion\"])\n",
    "\n",
    "# eliminar files que missing a totes les columnes\n",
    "sales_usa = sales_usa.dropna(how=\"all\")\n",
    "\n",
    "\n",
    "# seleccionem files que ens interessen per MODEL PREDICTIU\n",
    "sales_selected = sales_usa.select(\"Postal Code\", \"Category\", \"Sales\", \"Order Quantity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "## SHOPS ##\n",
    "###########\n",
    "# canviem nom columnes perque no ens deixa accedir-hi si tenen caracters especials\n",
    "# geometry.x\n",
    "new_column_name = \"geometry_x\"\n",
    "old_column_name = \"geometry.x\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# geometry.y\n",
    "new_column_name = \"geometry_y\"\n",
    "old_column_name = \"geometry.y\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.shop\n",
    "new_column_name = \"shop\"\n",
    "old_column_name = \"attributes.shop\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.name\n",
    "new_column_name = \"name\"\n",
    "old_column_name = \"attributes.name\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.osm_id2\n",
    "new_column_name = \"index\"\n",
    "old_column_name = \"attributes.osm_id2\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.addr_postcode\n",
    "new_column_name = \"postcode\"\n",
    "old_column_name = \"attributes.addr_postcode\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# ens quedem només amb columnes seleccionades\n",
    "selected_columns = ['geometry_x', 'geometry_y', \"shop\", \"name\", \"index\", \"postcode\"]\n",
    "shops_selected = shops.select(selected_columns)\n",
    "\n",
    "# eliminar files que tinguin missings --> excepte les que tenen missings a postcode!!\n",
    "shops_selected = shops_selected.filter(~(col(\"geometry_x\").isNull() |\n",
    "                                col(\"geometry_y\").isNull() |\n",
    "                                col(\"shop\").isNull() |\n",
    "                                col(\"name\").isNull() |\n",
    "                                col(\"index\").isNull()))\n",
    "\n",
    "# seleccionem files que ens interessen per MODEL PREDICTIU\n",
    "shops_selected = shops.select(\"shop\", \"postcode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, count, desc, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Calcular la media de ingresos por código postal\n",
    "income_avg = income_data.groupBy(\"zipcode\").agg(avg(\"income\").alias(\"avg_income\"))\n",
    "\n",
    "# Determinar las tiendas más comunes por código postal\n",
    "window = Window.partitionBy(\"zipcode\").orderBy(desc(\"count\"))\n",
    "top_shops = sales_data.groupBy(\"zipcode\", \"shop\").agg(count(\"*\").alias(\"count\")).\\\n",
    "    withColumn(\"rn\", row_number().over(window)).filter(col(\"rn\") <= 5)\n",
    "\n",
    "# Calcular métricas de ventas por código postal y tienda\n",
    "sales_metrics = sales_data.groupBy(\"zipcode\", \"shop\", \"category\").\\\n",
    "    agg(count(\"*\").alias(\"count\"), avg(\"sales\").alias(\"avg_sales\"), avg(\"profit\").alias(\"avg_profit\"),\n",
    "        avg(\"num_units\").alias(\"avg_num_units\"))\n",
    "\n",
    "# Combinar todas las métricas en una sola tabla\n",
    "result_table = sales_metrics.join(income_avg, \"zipcode\", \"left\").\\\n",
    "    join(top_shops, [\"zipcode\", \"shop\"], \"left\")\n",
    "\n",
    "# Mostrar el resultado\n",
    "result_table.show()\n",
    "\n",
    "# Finalizar la sesión de Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalitzar sessió de Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
