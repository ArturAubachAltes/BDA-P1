{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploitation Zone - Model Predictiu\n",
    "\n",
    "- exploitation zone del model predictiu\n",
    "- preparation pipeline per taula d'entrenament del model --> cada zipcode és un indiv\n",
    "    - Sales: 5 categories més comunes per zipcode, count vendes per zipcode, profit mitja per zipcode, mitjana num unitat per comanda per zipcode\n",
    "    - Shops: 5shops més comunes per zipcode\n",
    "    - Income: mitjana income per zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark\n",
    "#!pip install delta-spark\n",
    "\n",
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "#!wget -O \"HR_comma_sep.csv\" \"https://mydisk.cs.upc.edu/s/3o33yciBHADiFCD/download/HR_comma_sep.csv\"\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"Shops_Deltalake\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arxiu Parquet (de moment suposem aixo despres arreglem amb duckdb)\n",
    "shops = spark.read.parquet(\"./datalake/shops_data/2024-04-24_shops_data.parquet\")\n",
    "income = spark.read.parquet(\"./datalake/income_data/2024-04-24_IRSIncomeByZipCode_NoStateTotalsNoSmallZips.parquet\")\n",
    "sales = spark.read.parquet(\"./datalake/sales_data/2024-04-24_SuperstoreSalesTraining.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "## INCOME ##\n",
    "############\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Lista de todos los caracteres inválidos que quieres reemplazar o eliminar\n",
    "invalid_chars = [' ', ';', '{', '}', '(', ')', '\\n', '\\t', '=']\n",
    "\n",
    "# Función para limpiar los nombres de las columnas reemplazando los caracteres no válidos\n",
    "def clean_column_name(column_name):\n",
    "    for invalid_char in invalid_chars:\n",
    "        column_name = column_name.replace(invalid_char, \"_\")  # Reemplaza por subrayado o cualquier otro caracter válido que prefieras\n",
    "    return column_name\n",
    "\n",
    "# Aplicar la función de limpieza a cada columna\n",
    "cleaned_income = income.select([col(c).alias(clean_column_name(c)) for c in income.columns])\n",
    "\n",
    "# seleccionem files que ens interessen per MODEL PREDICTIU\n",
    "income_selected = cleaned_income.select(\"ZIPCODE\", \"Total_income_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "## SALES ##\n",
    "###########\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Lista de todos los caracteres inválidos que quieres reemplazar o eliminar\n",
    "invalid_chars = [' ', ';', '{', '}', '(', ')', '\\n', '\\t', '=']\n",
    "\n",
    "# Función para limpiar los nombres de las columnas reemplazando los caracteres no válidos\n",
    "def clean_column_name(column_name):\n",
    "    for invalid_char in invalid_chars:\n",
    "        column_name = column_name.replace(invalid_char, \"_\")  # Reemplaza por subrayado o cualquier otro caracter válido que prefieras\n",
    "    return column_name\n",
    "\n",
    "# Aplicar la función de limpieza a cada columna\n",
    "cleaned_sales = sales.select([col(c).alias(clean_column_name(c)) for c in sales.columns])\n",
    "\n",
    "# filtrar EEUU\n",
    "sales_usa = cleaned_sales.filter(col(\"Country_/_Region\") == \"United States of America\")\n",
    "\n",
    "# eliminar row\n",
    "sales_usa = sales_usa.dropDuplicates(subset=[col for col in sales_usa.columns if col != \"row\"])\n",
    "\n",
    "# eliminar customer_name\n",
    "sales_usa = sales_usa.drop(\"Customer_Name\")\n",
    "\n",
    "# no fa falta fer imputació de missings perque quan filtrem per USA no ens queden columnes amb missings\n",
    "\n",
    "# eliminar missings a postal_code\n",
    "sales_usa = sales_usa.dropna(subset=[\"Postal_Code\"])\n",
    "\n",
    "# eliminar missings a subregions\n",
    "sales_usa = sales_usa.dropna(subset=[\"SubRegion\"])\n",
    "\n",
    "# eliminar files que missing a totes les columnes\n",
    "sales_usa = sales_usa.dropna(how=\"all\")\n",
    "\n",
    "\n",
    "# seleccionem files que ens interessen per MODEL PREDICTIU\n",
    "sales_selected = sales_usa.select(\"Postal_Code\", \"Category\", \"Sales\", \"Order_Quantity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "## SHOPS ##\n",
    "###########\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# canviem nom columnes perque no ens deixa accedir-hi si tenen caracters especials\n",
    "# geometry.x\n",
    "new_column_name = \"geometry_x\"\n",
    "old_column_name = \"geometry.x\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# geometry.y\n",
    "new_column_name = \"geometry_y\"\n",
    "old_column_name = \"geometry.y\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.shop\n",
    "new_column_name = \"shop\"\n",
    "old_column_name = \"attributes.shop\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.name\n",
    "new_column_name = \"name\"\n",
    "old_column_name = \"attributes.name\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.osm_id2\n",
    "new_column_name = \"index\"\n",
    "old_column_name = \"attributes.osm_id2\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# attributes.addr_postcode\n",
    "new_column_name = \"postcode\"\n",
    "old_column_name = \"attributes.addr_postcode\"\n",
    "shops = shops.withColumnRenamed(old_column_name, new_column_name)\n",
    "\n",
    "# ens quedem només amb columnes seleccionades\n",
    "selected_columns = ['geometry_x', 'geometry_y', \"shop\", \"name\", \"index\", \"postcode\"]\n",
    "shops_selected = shops.select(selected_columns)\n",
    "\n",
    "# eliminar files que tinguin missings --> excepte les que tenen missings a postcode!!\n",
    "shops_selected = shops_selected.filter(~(col(\"geometry_x\").isNull() |\n",
    "                                col(\"geometry_y\").isNull() |\n",
    "                                col(\"shop\").isNull() |\n",
    "                                col(\"name\").isNull() |\n",
    "                                col(\"index\").isNull()))\n",
    "\n",
    "# seleccionem files que ens interessen per MODEL PREDICTIU\n",
    "shops_selected = shops.select(\"shop\", \"postcode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+-----+-----+-----+-----+-----+----------+----------+----------+----------+----------+-----------------+----------------------+------------------------------+\n",
      "|zipcode|avg_income_per_zipcode|shop1|shop2|shop3|shop4|shop5|sales_cat1|sales_cat2|sales_cat3|sales_cat4|sales_cat5|sales_per_zipcode|avg_profit_per_zipcode|avg_order_quantity_per_zipcode|\n",
      "+-------+----------------------+-----+-----+-----+-----+-----+----------+----------+----------+----------+----------+-----------------+----------------------+------------------------------+\n",
      "+-------+----------------------+-----+-----+-----+-----+-----+----------+----------+----------+----------+----------+-----------------+----------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creem dataframe buit amb les columnes que volem que tingui\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# definim estructura\n",
    "schema = StructType([\n",
    "    StructField(\"zipcode\", StringType(), True),\n",
    "    StructField(\"avg_income_per_zipcode\", DoubleType(), True),\n",
    "    StructField(\"shop1\", StringType(), True),\n",
    "    StructField(\"shop2\", StringType(), True),\n",
    "    StructField(\"shop3\", StringType(), True),\n",
    "    StructField(\"shop4\", StringType(), True),\n",
    "    StructField(\"shop5\", StringType(), True),\n",
    "    StructField(\"sales_cat1\", StringType(), True),\n",
    "    StructField(\"sales_cat2\", StringType(), True),\n",
    "    StructField(\"sales_cat3\", StringType(), True),\n",
    "    StructField(\"sales_cat4\", StringType(), True),\n",
    "    StructField(\"sales_cat5\", StringType(), True),\n",
    "    StructField(\"sales_per_zipcode\", IntegerType(), True),\n",
    "    StructField(\"avg_profit_per_zipcode\", DoubleType(), True),\n",
    "    StructField(\"avg_order_quantity_per_zipcode\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# crear dataframe buit\n",
    "df_model = spark.createDataFrame([], schema)\n",
    "\n",
    "df_model.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|zipcode|            shop|\n",
      "+-------+----------------+\n",
      "|  28000|         florist|\n",
      "|  28000|        copyshop|\n",
      "|  28000|     supermarket|\n",
      "|  28000|        boutique|\n",
      "|  28000|    dry_cleaning|\n",
      "|  28017|         clothes|\n",
      "|  28017|     hairdresser|\n",
      "|  28017|          beauty|\n",
      "|  28017|     convenience|\n",
      "|  28017|           paint|\n",
      "|  28020|      car_repair|\n",
      "|  28035|           shoes|\n",
      "|  28035|      stationery|\n",
      "|  28035|         laundry|\n",
      "|  28035|           paint|\n",
      "|  28035|     convenience|\n",
      "|  28040|department_store|\n",
      "|  28450|          coffee|\n",
      "|  28459|     convenience|\n",
      "|  28459|             yes|\n",
      "+-------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Definir una ventana sobre la cual aplicar row_number()\n",
    "window = Window.partitionBy(\"zipcode\").orderBy(F.desc(\"count\"))\n",
    "\n",
    "# Calcular la cantidad de cada tipo de tienda para cada código postal\n",
    "top_shops = (\n",
    "    shops_selected.groupBy(\"zipcode\", \"shop\")\n",
    "         .count()\n",
    "         .withColumn(\"row_num\", F.row_number().over(window))\n",
    "         .filter(F.col(\"row_num\") <= 5)\n",
    "         .select(\"zipcode\", \"shop\", \"count\")\n",
    "         .orderBy(\"zipcode\", \"row_num\")\n",
    ")\n",
    "\n",
    "top_shops = top_shops.drop(\"count\")\n",
    "\n",
    "# Mostrar los resultados\n",
    "#top_shops.show()\n",
    "\n",
    "shops_cleaned = top_shops.dropna()\n",
    "shops_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+\n",
      "|zipcode|avg_income_per_zipcode|\n",
      "+-------+----------------------+\n",
      "|  35004|                258024|\n",
      "|  35005|                129390|\n",
      "|  35006|                 58585|\n",
      "|  35007|                651350|\n",
      "|  35010|                382106|\n",
      "|  35014|                 67885|\n",
      "|  35016|                333226|\n",
      "|  35019|                 35392|\n",
      "|  35020|                262475|\n",
      "|  35022|                521539|\n",
      "|  35023|                480458|\n",
      "|  35031|                112152|\n",
      "|  35033|                 67437|\n",
      "|  35034|                 52030|\n",
      "|  35035|                 31542|\n",
      "|  35040|                359868|\n",
      "|  35042|                 96503|\n",
      "|  35043|                363943|\n",
      "|  35044|                124406|\n",
      "|  35045|                236772|\n",
      "+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# taula a partir de income\n",
    "df_model = income_selected\n",
    "\n",
    "# renombrem columnes\n",
    "df_model = df_model.withColumnRenamed(\"ZIPCODE\", \"zipcode\")\n",
    "df_model = df_model.withColumnRenamed(\"Total_income_amount\", \"avg_income_per_zipcode\")\n",
    "\n",
    "# renombrem columnes de zipcode a sales i shops per poder fer join\n",
    "shops_selected = shops_selected.withColumnRenamed(\"postcode\", \"zipcode\")\n",
    "sales_selected = sales_selected.withColumnRenamed(\"Postal_Code\", \"zipcode\")\n",
    "\n",
    "df_model.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+-------------+\n",
      "|zipcode|avg_income_per_zipcode|         shop|\n",
      "+-------+----------------------+-------------+\n",
      "|  96701|               1369267|      charity|\n",
      "|  96701|               1369267|      bicycle|\n",
      "|  96701|               1369267|  supermarket|\n",
      "|  96706|               1657633|  supermarket|\n",
      "|  96706|               1657633|      chemist|\n",
      "|  96708|                205389|variety_store|\n",
      "|  96708|                205389|      hammock|\n",
      "|  96712|                209977|         gift|\n",
      "|  96712|                209977|       tattoo|\n",
      "|  96712|                209977|      jewelry|\n",
      "|  96712|                209977|       sports|\n",
      "|  96712|                209977|      clothes|\n",
      "|  96713|                 30221|          art|\n",
      "|  96720|               1091080|      clothes|\n",
      "|  96720|               1091080|      outdoor|\n",
      "|  96720|               1091080|          art|\n",
      "|  96720|               1091080|  health_food|\n",
      "|  96720|               1091080|  supermarket|\n",
      "|  96722|                 84923|  hairdresser|\n",
      "|  96722|                 84923|  convenience|\n",
      "+-------+----------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_data = df_model.join(top_shops, (df_model[\"zipcode\"] == top_shops[\"zipcode\"]), \"inner\")\n",
    "selected_columns = [df_model[col] for col in df_model.columns] + [top_shops[col] for col in top_shops.columns if col not in [\"zipcode\"]]\n",
    "joined_data = joined_data.select(selected_columns)\n",
    "joined_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas: 117\n",
      "Número de columnas: 3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cosetes aux 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el número de filas\n",
    "num_rows = df_model.count()\n",
    "\n",
    "# Obtener el número de columnas\n",
    "num_columns = len(df_model.columns)\n",
    "\n",
    "# Mostrar el número de filas y columnas\n",
    "print(\"Número de filas:\", num_rows)\n",
    "print(\"Número de columnas:\", num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+----+\n",
      "|zipcode|avg_income_per_zipcode|shop|\n",
      "+-------+----------------------+----+\n",
      "|  35004|                258024|NULL|\n",
      "|  35005|                129390|NULL|\n",
      "|  35006|                 58585|NULL|\n",
      "|  35007|                651350|NULL|\n",
      "|  35010|                382106|NULL|\n",
      "|  35014|                 67885|NULL|\n",
      "|  35016|                333226|NULL|\n",
      "|  35019|                 35392|NULL|\n",
      "|  35020|                262475|NULL|\n",
      "|  35022|                521539|NULL|\n",
      "|  35023|                480458|NULL|\n",
      "|  35031|                112152|NULL|\n",
      "|  35033|                 67437|NULL|\n",
      "|  35034|                 52030|NULL|\n",
      "|  35035|                 31542|NULL|\n",
      "|  35040|                359868|NULL|\n",
      "|  35042|                 96503|NULL|\n",
      "|  35043|                363943|NULL|\n",
      "|  35044|                124406|NULL|\n",
      "|  35045|                236772|NULL|\n",
      "+-------+----------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# taula a partir de income\n",
    "df_model = income_selected\n",
    "\n",
    "# renombrem columnes\n",
    "df_model = df_model.withColumnRenamed(\"ZIPCODE\", \"zipcode\")\n",
    "df_model = df_model.withColumnRenamed(\"Total_income_amount\", \"avg_income_per_zipcode\")\n",
    "\n",
    "# renombrem columnes de zipcode a sales i shops per poder fer join\n",
    "shops_selected = shops_selected.withColumnRenamed(\"postcode\", \"zipcode\")\n",
    "sales_selected = sales_selected.withColumnRenamed(\"Postal_Code\", \"zipcode\")\n",
    "\n",
    "# modificar taula shops per que tingui les columnes que volem --> shop1, shop2, shop3, shop4, shop5\n",
    "\n",
    "\n",
    "df_model = df_model.join(shops_selected, \"zipcode\", \"left\")\n",
    "\n",
    "\n",
    "# modificar taula sales perque tingui les columnes que volem --> sales_cat1, sales_cat2, sales_cat3, sales_cat4, sales_cat5, sales_per_zipcode, avg_profit_per_zipcode, avg_order_quantity_per_zipcode\n",
    "\n",
    "\n",
    "df_model = df_model.join(sales_selected, \"zipcode\", \"left\")\n",
    "\n",
    "\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "df_model.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduim valors de zipcode\n",
    "df_model = df_model.withColumn(\"zipcode\", income_selected[\"ZIPCODE\"])\n",
    "\n",
    "# unim income amb df_model\n",
    "df_model = df_model.join(income_selected, df_model.zipcode == income_selected.zipcode, how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, count, desc, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Calcular la media de ingresos por código postal\n",
    "income_avg = income_data.groupBy(\"zipcode\").agg(avg(\"income\").alias(\"avg_income\"))\n",
    "\n",
    "# Determinar las tiendas más comunes por código postal\n",
    "window = Window.partitionBy(\"zipcode\").orderBy(desc(\"count\"))\n",
    "top_shops = sales_data.groupBy(\"zipcode\", \"shop\").agg(count(\"*\").alias(\"count\")).\\\n",
    "    withColumn(\"rn\", row_number().over(window)).filter(col(\"rn\") <= 5)\n",
    "\n",
    "# Calcular métricas de ventas por código postal y tienda\n",
    "sales_metrics = sales_data.groupBy(\"zipcode\", \"shop\", \"category\").\\\n",
    "    agg(count(\"*\").alias(\"count\"), avg(\"sales\").alias(\"avg_sales\"), avg(\"profit\").alias(\"avg_profit\"),\n",
    "        avg(\"num_units\").alias(\"avg_num_units\"))\n",
    "\n",
    "# Combinar todas las métricas en una sola tabla\n",
    "result_table = sales_metrics.join(income_avg, \"zipcode\", \"left\").\\\n",
    "    join(top_shops, [\"zipcode\", \"shop\"], \"left\")\n",
    "\n",
    "# Mostrar el resultado\n",
    "result_table.show()\n",
    "\n",
    "# Finalizar la sesión de Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalitzar sessió de Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
